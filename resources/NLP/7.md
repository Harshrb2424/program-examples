# üí° **Ex.No: 7 ‚Äì Morphological Analysis, N-Grams & Smoothing using NLTK**

---

## üß™ (a) **Morphological Analysis using NLTK**

### ‚úÖ **AIM:**

To perform **morphological analysis** on a given text using the NLTK library.

---

### üìú **PROCEDURE:**

1. Import necessary libraries from `nltk`.
2. Tokenize the input text using `word_tokenize()`.
3. Use `pos_tag()` to assign POS tags.
4. Convert tags into WordNet-compatible tags.
5. Lemmatize the words using `WordNetLemmatizer`.

---

### üêç **PROGRAM:**

```python
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import wordnet
from nltk.stem import WordNetLemmatizer

# Download necessary datasets
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')

# Function to convert POS tag for lemmatizer
def get_wordnet_pos(word):
    tag = nltk.pos_tag([word])[0][1][0].upper()
    tag_dict = {"J": wordnet.ADJ, "N": wordnet.NOUN, "V": wordnet.VERB, "R": wordnet.ADV}
    return tag_dict.get(tag, wordnet.NOUN)

# Morphological Analysis
def morphological_analysis(text):
    tokens = word_tokenize(text)
    lemmatizer = WordNetLemmatizer()
    lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in tokens]
    return lemmatized_words

# Sample Input
text = "The running cats were quickly jumping over the fences."
lemmatized_result = morphological_analysis(text)
print("Original Text:", text)
print("Morphological Analysis (Lemmatized Text):", lemmatized_result)
```

---

### üì§ **OUTPUT:**

```
Original Text: The running cats were quickly jumping over the fences.
Morphological Analysis (Lemmatized Text): ['The', 'run', 'cat', 'be', 'quickly', 'jump', 'over', 'the', 'fence', '.']
```

---

### ‚úÖ **RESULT:**

Morphological analysis was successfully performed using **lemmatization** with POS tagging to convert words to their root forms.

---

## üß™ (b) **Generating N-Grams using NLTK**

### ‚úÖ **AIM:**

To generate **bigrams**, **trigrams**, and other n-grams from a given text using NLTK‚Äôs `ngrams` module.

---

### üìú **PROCEDURE:**

1. Tokenize the text using `word_tokenize()`.
2. Use `ngrams()` from `nltk.util` to generate n-grams.
3. Format and print the result.

---

### üêç **PROGRAM:**

```python
from nltk.tokenize import word_tokenize
from nltk.util import ngrams

def generate_ngrams(text, n):
    tokens = word_tokenize(text)
    n_grams = list(ngrams(tokens, n))
    return [" ".join(gram) for gram in n_grams]

# Sample Input
text = "The quick brown fox jumps over the lazy dog."
bigrams = generate_ngrams(text, 2)
trigrams = generate_ngrams(text, 3)

print("Bigrams:", bigrams)
print("Trigrams:", trigrams)
```

---

### üì§ **OUTPUT:**

```
Bigrams: ['The quick', 'quick brown', 'brown fox', 'fox jumps', 'jumps over', 'over the', 'the lazy', 'lazy dog', 'dog .']
Trigrams: ['The quick brown', 'quick brown fox', 'brown fox jumps', 'fox jumps over', 'jumps over the', 'over the lazy', 'the lazy dog', 'lazy dog .']
```

---

### ‚úÖ **RESULT:**

The program successfully generated **bigrams** and **trigrams** using the input text.

---

## üß™ (c) **Implementing N-Grams Smoothing (Laplace Smoothing)**

### ‚úÖ **AIM:**

To apply **Laplace Smoothing** to bigrams in order to handle zero probabilities in n-gram models.

---

### üìú **PROCEDURE:**

1. Tokenize text and generate **unigram** and **bigram** frequency counts.
2. Apply **Laplace Smoothing** using the formula:

   $$
   P(w_i | w_{i-1}) = \frac{C(w_{i-1}, w_i) + 1}{C(w_{i-1}) + V}
   $$

   where `V` is vocabulary size.

---

### üêç **PROGRAM:**

```python
from collections import Counter
from nltk.util import ngrams
from nltk.tokenize import word_tokenize

def laplace_smoothing(unigrams, bigrams, vocab_size):
    smoothed_probs = {}
    for bigram in bigrams:
        count_bigram = bigrams[bigram]
        count_unigram = unigrams[bigram[0]]
        prob = (count_bigram + 1) / (count_unigram + vocab_size)
        smoothed_probs[bigram] = prob
    return smoothed_probs

# Sample Input
text = "The cat sat on the mat. The dog sat on the rug."
tokens = word_tokenize(text)

# Count unigrams and bigrams
unigram_counts = Counter(tokens)
bigram_counts = Counter(ngrams(tokens, 2))
vocab_size = len(unigram_counts)

# Apply Laplace Smoothing
smoothed_bigram_probs = laplace_smoothing(unigram_counts, bigram_counts, vocab_size)

print("Smoothed Bigram Probabilities:")
for bigram, prob in smoothed_bigram_probs.items():
    print(f"P({bigram[1]} | {bigram[0]}) = {prob:.4f}")
```

---

### üì§ **OUTPUT (Sample):**

```
Smoothed Bigram Probabilities:
P(cat | The) = 0.1667
P(sat | cat) = 0.1429
P(on | sat) = 0.1429
P(the | on) = 0.1667
P(mat | the) = 0.0909
...
```

---

### ‚úÖ **RESULT:**

**Laplace Smoothing** was successfully implemented for bigram probabilities. This ensures that no bigram has a probability of zero, making the model more robust.

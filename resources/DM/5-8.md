# 5
### create_csv.py
``` python
import pandas as pd
import numpy as np

# Create dataset
data = {
    "EmpID": range(101, 126),
    "Name": [
        "Alice", "Bob", "Charlie", "David", "Eve",
        "Frank", "Grace", "Helen", "Ivy", "Jack",
        "Karen", "Leo", "Mona", "Nina", "Oscar",
        "Paul", "Queen", "Raj", "Sam", "Tina",
        "Uma", "Victor", "Wendy", "Xavier", "Yara"
    ],
    "Department": [
        "HR", "Finance", "IT", "IT", "Finance",
        "HR", "Finance", "IT", "Finance", "HR",
        "IT", "Finance", "HR", "Finance", "IT",
        "HR", "Finance", "Finance", "IT", "HR",
        "Finance", "IT", "Finance", "HR", "IT"
    ],
    "Salary": [
        35000, 45000, 60000, 50000, np.nan,
        42000, 39000, 70000, 48000, 41000,
        55000, 58000, 36000, np.nan, 47000,
        40000, 53000, 61000, 30000, 46000,
        49500, 72000, 39000, 38000, 51000
    ],
    "Experience(Years)": [
        1, 3, 7, 5, 2,
        4, 2, 10, 6, 3,
        8, 9, 1, 2, 6,
        3, 7, 12, 1, 5,
        4, 11, 2, 2, 8
    ]
}

# Save to CSV
df_raw = pd.DataFrame(data)
df_raw.to_csv("employees_raw.csv", index=False)

print("✅ employees_raw.csv created successfully with 25 rows.")

```

### etl_main.py
```python
import pandas as pd

# -------------------------------
# Step 1: Extract
# -------------------------------
df = pd.read_csv("employees_raw.csv")
print("=== RAW DATA (Extracted) ===")
print(df.head(10))

# -------------------------------
# Step 2: Transform
# -------------------------------

# Fill missing salary with mean
df["Salary"].fillna(df["Salary"].mean(), inplace=True)

# Standardize department names to uppercase
df["Department"] = df["Department"].str.upper()

# Filter rows with Salary > 40000
df_filtered = df[df["Salary"] > 40000]

# Add Experience Level column
df_filtered["ExperienceLevel"] = pd.cut(
    df_filtered["Experience(Years)"],
    bins=[0, 3, 7, 20],
    labels=["Junior", "Mid-level", "Senior"]
)

print("\n=== TRANSFORMED DATA ===")
print(df_filtered.head(10))

# -------------------------------
# Step 3: Load
# -------------------------------
df_filtered.to_csv("employees_clean.csv", index=False)

print("\n✅ Clean data has been saved to 'employees_clean.csv'")
print(f"Total Rows Before: {len(df)} | After Transformation: {len(df_filtered)}")

```

### output
```bash
> python 5.py
=== RAW DATA (Extracted) ===
   EmpID     Name Department   Salary  Experience(Years)
0    101    Alice         HR  35000.0                  1
1    102      Bob    Finance  45000.0                  3
2    103  Charlie         IT  60000.0                  7
3    104    David         IT  50000.0                  5
4    105      Eve    Finance      NaN                  2
5    106    Frank         HR  42000.0                  4
6    107    Grace    Finance  39000.0                  2
7    108    Helen         IT  70000.0                 10
8    109      Ivy    Finance  48000.0                  6
9    110     Jack         HR  41000.0                  3

=== TRANSFORMED DATA ===
    EmpID     Name Department        Salary  Experience(Years) ExperienceLevel
1     102      Bob    FINANCE  45000.000000                  3          Junior
2     103  Charlie         IT  60000.000000                  7       Mid-level
    EmpID     Name Department        Salary  Experience(Years) ExperienceLevel
1     102      Bob    FINANCE  45000.000000                  3          Junior
2     103  Charlie         IT  60000.000000                  7       Mid-level
1     102      Bob    FINANCE  45000.000000                  3          Junior
2     103  Charlie         IT  60000.000000                  7       Mid-level
2     103  Charlie         IT  60000.000000                  7       Mid-level
3     104    David         IT  50000.000000                  5       Mid-level
4     105      Eve    FINANCE  48065.217391                  2          Junior
5     106    Frank         HR  42000.000000                  4       Mid-level
7     108    Helen         IT  70000.000000                 10          Senior
8     109      Ivy    FINANCE  48000.000000                  6       Mid-level
4     105      Eve    FINANCE  48065.217391                  2          Junior
5     106    Frank         HR  42000.000000                  4       Mid-level
7     108    Helen         IT  70000.000000                 10          Senior
8     109      Ivy    FINANCE  48000.000000                  6       Mid-level
5     106    Frank         HR  42000.000000                  4       Mid-level
7     108    Helen         IT  70000.000000                 10          Senior
8     109      Ivy    FINANCE  48000.000000                  6       Mid-level
7     108    Helen         IT  70000.000000                 10          Senior
8     109      Ivy    FINANCE  48000.000000                  6       Mid-level
8     109      Ivy    FINANCE  48000.000000                  6       Mid-level
9     110     Jack         HR  41000.000000                  3          Junior
9     110     Jack         HR  41000.000000                  3          Junior
10    111    Karen         IT  55000.000000                  8          Senior
11    112      Leo    FINANCE  58000.000000                  9          Senior

✅ Clean data has been saved to 'employees_clean.csv'
Total Rows Before: 25 | After Transformation: 18
```
# 6
### create_aoi_csv.py
```python
import pandas as pd

# Sample dataset with attributes for generalization
data = {
    "StudentID": range(1, 11),
    "Name": ["Alice", "Bob", "Charlie", "David", "Eve",
             "Frank", "Grace", "Helen", "Ivy", "Jack"],
    "City": ["Hyderabad", "Hyderabad", "Delhi", "Delhi", "Mumbai",
             "Pune", "Pune", "Chennai", "Chennai", "Hyderabad"],
    "Age": [21, 25, 19, 32, 45, 28, 22, 35, 41, 18],
    "Department": ["CSE", "CSE", "ECE", "ECE", "MECH",
                   "CSE", "EEE", "MECH", "EEE", "CSE"]
}

# Save to CSV
df_raw = pd.DataFrame(data)
df_raw.to_csv("aoi_raw.csv", index=False)

print("✅ aoi_raw.csv created successfully with 10 rows.")

```

### ### `aoi_main.py`
```python
import pandas as pd

# -------------------------------
# Step 1: Extract
# -------------------------------
df = pd.read_csv("aoi_raw.csv")
print("=== RAW DATA ===")
print(df)

# -------------------------------
# Step 2: Define Concept Hierarchies
# -------------------------------
city_to_state = {
    "Hyderabad": "Telangana",
    "Delhi": "Delhi",
    "Mumbai": "Maharashtra",
    "Pune": "Maharashtra",
    "Chennai": "Tamil Nadu"
}

state_to_country = {
    "Telangana": "India",
    "Delhi": "India",
    "Maharashtra": "India",
    "Tamil Nadu": "India"
}

def generalize_age(age):
    if age <= 20:
        return "Teen"
    elif 21 <= age <= 30:
        return "Young"
    elif 31 <= age <= 40:
        return "Adult"
    else:
        return "Senior"

# -------------------------------
# Step 3: Apply AOI (Generalization)
# -------------------------------
df["State"] = df["City"].map(city_to_state)
df["Country"] = df["State"].map(state_to_country)
df["AgeGroup"] = df["Age"].apply(generalize_age)

# Drop detailed columns (low-level)
df_generalized = df.drop(columns=["City", "Age"])

print("\n=== GENERALIZED DATA (AOI Result) ===")
print(df_generalized)

# -------------------------------
# Step 4: Load/Save Result
# -------------------------------
df_generalized.to_csv("aoi_generalized.csv", index=False)
print("\n✅ Generalized data saved to 'aoi_generalized.csv'")

```
### output
```bash
> python 6.py
=== RAW DATA ===
   StudentID     Name       City  Age Department
0          1    Alice  Hyderabad   21        CSE
1          2      Bob  Hyderabad   25        CSE
2          3  Charlie      Delhi   19        ECE
3          4    David      Delhi   32        ECE
4          5      Eve     Mumbai   45       MECH
5          6    Frank       Pune   28        CSE
6          7    Grace       Pune   22        EEE
7          8    Helen    Chennai   35       MECH
8          9      Ivy    Chennai   41        EEE
9         10     Jack  Hyderabad   18        CSE

=== GENERALIZED DATA (AOI Result) ===
   StudentID     Name Department        State Country AgeGroup
0          1    Alice        CSE    Telangana   India    Young
1          2      Bob        CSE    Telangana   India    Young
2          3  Charlie        ECE        Delhi   India     Teen
3          4    David        ECE        Delhi   India    Adult
4          5      Eve       MECH  Maharashtra   India   Senior
5          6    Frank        CSE  Maharashtra   India    Young
6          7    Grace        EEE  Maharashtra   India    Young
7          8    Helen       MECH   Tamil Nadu   India    Adult
8          9      Ivy        EEE   Tamil Nadu   India   Senior
9         10     Jack        CSE    Telangana   India     Teen

✅ Generalized data saved to 'aoi_generalized.csv'
```

# 7
### create_apriori_csv.py
```python
import pandas as pd

# Sample supermarket transactions (each row = transaction)
data = {
    "TID": range(1, 11),
    "Items": [
        "Milk,Bread,Butter",
        "Milk,Bread",
        "Milk,Diaper,Beer,Eggs",
        "Bread,Butter",
        "Milk,Bread,Diaper,Beer",
        "Milk,Bread,Diaper,Butter",
        "Beer,Diaper",
        "Milk,Bread,Butter,Eggs",
        "Milk,Bread,Diaper,Beer,Butter",
        "Bread,Diaper,Beer"
    ]
}

df = pd.DataFrame(data)
df.to_csv("transactions.csv", index=False)

print("✅ transactions.csv created successfully with 10 rows.")

```

### apriori_main.py
```python
import pandas as pd
from mlxtend.preprocessing import TransactionEncoder
from mlxtend.frequent_patterns import apriori, association_rules

# -------------------------------
# Step 1: Extract
# -------------------------------
df = pd.read_csv("transactions.csv")
print("=== RAW TRANSACTIONS ===")
print(df)

# Convert "Items" column into list of lists
transactions = df["Items"].apply(lambda x: x.split(",")).tolist()

# -------------------------------
# Step 2: Transform (One-hot encoding)
# -------------------------------
te = TransactionEncoder()
te_ary = te.fit(transactions).transform(transactions)
df_trans = pd.DataFrame(te_ary, columns=te.columns_)

print("\n=== TRANSACTIONS ONE-HOT ENCODED ===")
print(df_trans.head())

# -------------------------------
# Step 3: Apply Apriori
# -------------------------------
frequent_itemsets = apriori(df_trans, min_support=0.3, use_colnames=True)

print("\n=== FREQUENT ITEMSETS (min_support=0.3) ===")
print(frequent_itemsets)

# -------------------------------
# Step 4: Generate Association Rules
# -------------------------------
rules = association_rules(frequent_itemsets, metric="confidence", min_threshold=0.6)

print("\n=== ASSOCIATION RULES ===")
print(rules[["antecedents", "consequents", "support", "confidence", "lift"]])

# -------------------------------
# Step 5: Save Results
# -------------------------------
frequent_itemsets.to_csv("frequent_itemsets.csv", index=False)
rules.to_csv("association_rules.csv", index=False)

print("\n✅ Results saved to 'frequent_itemsets.csv' and 'association_rules.csv'")

```
### output
```bash
> python 7.py
=== RAW TRANSACTIONS ===
   TID                          Items
0    1              Milk,Bread,Butter
1    2                     Milk,Bread
2    3          Milk,Diaper,Beer,Eggs
3    4                   Bread,Butter
4    5         Milk,Bread,Diaper,Beer
5    6       Milk,Bread,Diaper,Butter
6    7                    Beer,Diaper
7    8         Milk,Bread,Butter,Eggs
8    9  Milk,Bread,Diaper,Beer,Butter
9   10              Bread,Diaper,Beer

=== TRANSACTIONS ONE-HOT ENCODED ===
    Beer  Bread  Butter  Diaper   Eggs   Milk
0  False   True    True   False  False   True
1  False   True   False   False  False   True
2   True  False   False    True   True   True
3  False   True    True   False  False  False
4   True   True   False    True  False   True

=== FREQUENT ITEMSETS (min_support=0.3) ===
    support               itemsets
0       0.5                 (Beer)
1       0.8                (Bread)
2       0.5               (Butter)
3       0.6               (Diaper)
4       0.7                 (Milk)
5       0.3          (Bread, Beer)
6       0.5         (Diaper, Beer)
7       0.3           (Milk, Beer)
8       0.5        (Butter, Bread)
9       0.4        (Diaper, Bread)
10      0.6          (Milk, Bread)
11      0.4         (Milk, Butter)
12      0.4         (Milk, Diaper)
13      0.3  (Diaper, Bread, Beer)
14      0.3   (Milk, Diaper, Beer)
15      0.4  (Milk, Butter, Bread)
16      0.3  (Milk, Diaper, Bread)

=== ASSOCIATION RULES ===
        antecedents      consequents  support  confidence      lift
0            (Beer)          (Bread)      0.3    0.600000  0.750000
1          (Diaper)           (Beer)      0.5    0.833333  1.666667
2            (Beer)         (Diaper)      0.5    1.000000  1.666667
3            (Beer)           (Milk)      0.3    0.600000  0.857143
4          (Butter)          (Bread)      0.5    1.000000  1.250000
5           (Bread)         (Butter)      0.5    0.625000  1.250000
6          (Diaper)          (Bread)      0.4    0.666667  0.833333
7            (Milk)          (Bread)      0.6    0.857143  1.071429
8           (Bread)           (Milk)      0.6    0.750000  1.071429
9          (Butter)           (Milk)      0.4    0.800000  1.142857
10         (Diaper)           (Milk)      0.4    0.666667  0.952381
11  (Diaper, Bread)           (Beer)      0.3    0.750000  1.500000
12   (Diaper, Beer)          (Bread)      0.3    0.600000  0.750000
13    (Beer, Bread)         (Diaper)      0.3    1.000000  1.666667
14           (Beer)  (Diaper, Bread)      0.3    0.600000  1.500000
15   (Milk, Diaper)           (Beer)      0.3    0.750000  1.500000
16     (Milk, Beer)         (Diaper)      0.3    1.000000  1.666667
17   (Diaper, Beer)           (Milk)      0.3    0.600000  0.857143
18           (Beer)   (Milk, Diaper)      0.3    0.600000  1.500000
19   (Milk, Butter)          (Bread)      0.4    1.000000  1.250000
20    (Milk, Bread)         (Butter)      0.4    0.666667  1.333333
21  (Butter, Bread)           (Milk)      0.4    0.800000  1.142857
22         (Butter)    (Milk, Bread)      0.4    0.800000  1.333333
23   (Milk, Diaper)          (Bread)      0.3    0.750000  0.937500
24  (Diaper, Bread)           (Milk)      0.3    0.750000  1.071429

✅ Results saved to 'frequent_itemsets.csv' and 'association_rules.csv'
```

# 8
### create_fp_csv.py
```python
import pandas as pd

# Sample supermarket transactions (10 rows)
data = {
    "TID": range(1, 11),
    "Items": [
        "Milk,Bread,Butter",
        "Milk,Bread",
        "Milk,Diaper,Beer,Eggs",
        "Bread,Butter",
        "Milk,Bread,Diaper,Beer",
        "Milk,Bread,Diaper,Butter",
        "Beer,Diaper",
        "Milk,Bread,Butter,Eggs",
        "Milk,Bread,Diaper,Beer,Butter",
        "Bread,Diaper,Beer"
    ]
}

df = pd.DataFrame(data)
df.to_csv("fp_transactions.csv", index=False)

print("✅ fp_transactions.csv created successfully with 10 transactions.")

```

### fp_growth_main.py
```python
import pandas as pd
from mlxtend.preprocessing import TransactionEncoder
from mlxtend.frequent_patterns import fpgrowth, association_rules

# -------------------------------
# Step 1: Extract
# -------------------------------
df = pd.read_csv("fp_transactions.csv")
print("=== RAW TRANSACTIONS ===")
print(df)

# Convert "Items" column into list of lists
transactions = df["Items"].apply(lambda x: x.split(",")).tolist()

# -------------------------------
# Step 2: Transform (One-hot encoding)
# -------------------------------
te = TransactionEncoder()
te_ary = te.fit(transactions).transform(transactions)
df_trans = pd.DataFrame(te_ary, columns=te.columns_)

print("\n=== TRANSACTIONS ONE-HOT ENCODED ===")
print(df_trans.head())

# -------------------------------
# Step 3: Apply FP-Growth
# -------------------------------
frequent_itemsets = fpgrowth(df_trans, min_support=0.3, use_colnames=True)

print("\n=== FREQUENT ITEMSETS (min_support=0.3) ===")
print(frequent_itemsets)

# -------------------------------
# Step 4: Generate Association Rules
# -------------------------------
rules = association_rules(frequent_itemsets, metric="confidence", min_threshold=0.6)

print("\n=== ASSOCIATION RULES ===")
print(rules[["antecedents", "consequents", "support", "confidence", "lift"]])

# -------------------------------
# Step 5: Save Results
# -------------------------------
frequent_itemsets.to_csv("fp_frequent_itemsets.csv", index=False)
rules.to_csv("fp_association_rules.csv", index=False)

print("\n✅ Results saved to 'fp_frequent_itemsets.csv' and 'fp_association_rules.csv'")

```

### output
```bash
> python 8.py
=== RAW TRANSACTIONS ===
   TID                          Items
0    1              Milk,Bread,Butter
1    2                     Milk,Bread
2    3          Milk,Diaper,Beer,Eggs
3    4                   Bread,Butter
4    5         Milk,Bread,Diaper,Beer
5    6       Milk,Bread,Diaper,Butter
6    7                    Beer,Diaper
7    8         Milk,Bread,Butter,Eggs
8    9  Milk,Bread,Diaper,Beer,Butter
9   10              Bread,Diaper,Beer

=== TRANSACTIONS ONE-HOT ENCODED ===
    Beer  Bread  Butter  Diaper   Eggs   Milk
0  False   True    True   False  False   True
1  False   True   False   False  False   True
2   True  False   False    True   True   True
3  False   True    True   False  False  False
4   True   True   False    True  False   True

=== FREQUENT ITEMSETS (min_support=0.3) ===
    support               itemsets
0       0.8                (Bread)
1       0.7                 (Milk)
2       0.5               (Butter)
3       0.6               (Diaper)
4       0.5                 (Beer)
5       0.6          (Bread, Milk)
6       0.5        (Bread, Butter)
7       0.4         (Milk, Butter)
8       0.4  (Bread, Milk, Butter)
9       0.4         (Diaper, Milk)
10      0.4        (Bread, Diaper)
11      0.3  (Bread, Diaper, Milk)
12      0.5         (Beer, Diaper)
13      0.3           (Beer, Milk)
14      0.3          (Beer, Bread)
15      0.3   (Beer, Diaper, Milk)
16      0.3  (Beer, Bread, Diaper)

=== ASSOCIATION RULES ===
        antecedents      consequents  support  confidence      lift
0           (Bread)           (Milk)      0.6    0.750000  1.071429
1            (Milk)          (Bread)      0.6    0.857143  1.071429
2           (Bread)         (Butter)      0.5    0.625000  1.250000
3          (Butter)          (Bread)      0.5    1.000000  1.250000
4          (Butter)           (Milk)      0.4    0.800000  1.142857
5     (Bread, Milk)         (Butter)      0.4    0.666667  1.333333
6   (Bread, Butter)           (Milk)      0.4    0.800000  1.142857
7    (Milk, Butter)          (Bread)      0.4    1.000000  1.250000
8          (Butter)    (Bread, Milk)      0.4    0.800000  1.333333
9          (Diaper)           (Milk)      0.4    0.666667  0.952381
10         (Diaper)          (Bread)      0.4    0.666667  0.833333
11  (Bread, Diaper)           (Milk)      0.3    0.750000  1.071429
12   (Diaper, Milk)          (Bread)      0.3    0.750000  0.937500
13           (Beer)         (Diaper)      0.5    1.000000  1.666667
14         (Diaper)           (Beer)      0.5    0.833333  1.666667
15           (Beer)           (Milk)      0.3    0.600000  0.857143
16           (Beer)          (Bread)      0.3    0.600000  0.750000
17   (Beer, Diaper)           (Milk)      0.3    0.600000  0.857143
18     (Beer, Milk)         (Diaper)      0.3    1.000000  1.666667
19   (Diaper, Milk)           (Beer)      0.3    0.750000  1.500000
20           (Beer)   (Diaper, Milk)      0.3    0.600000  1.500000
21    (Beer, Bread)         (Diaper)      0.3    1.000000  1.666667
22   (Beer, Diaper)          (Bread)      0.3    0.600000  0.750000
23  (Bread, Diaper)           (Beer)      0.3    0.750000  1.500000
24           (Beer)  (Bread, Diaper)      0.3    0.600000  1.500000

✅ Results saved to 'fp_frequent_itemsets.csv' and 'fp_association_rules.csv'
```
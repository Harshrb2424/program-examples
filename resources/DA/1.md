Hereâ€™s a complete and structured response for SET-1, including detailed explanation, code, and formatted sections for report or assignment submission.


---

âœ… SET-1


---

1. Data Preprocessing


---

ðŸ”¹ a. Handling Missing Values

Aim:

To identify and manage missing data in a dataset to ensure completeness and improve analysis accuracy.

Description:

Missing data can affect the performance of machine learning models and statistical analysis. These may arise due to errors in data collection or system faults. Techniques to handle missing data include deletion, imputation, or substitution.

Procedure:

1. Load the dataset.


2. Check for missing values.


3. Choose an appropriate method to handle them (drop or fill).


4. Apply the method and verify results.



Program:

import pandas as pd
import numpy as np

# Sample dataset
data = {
    'Name': ['Amit', 'Neha', 'John', 'Sara', 'Kiran'],
    'Age': [25, np.nan, 30, 22, np.nan],
    'Marks': [80, 90, np.nan, 85, 88]
}
df = pd.DataFrame(data)

print("Original Data:\n", df)

# Detect missing values
print("\nMissing Values:\n", df.isnull().sum())

# Fill missing values using mean
df['Age'].fillna(df['Age'].mean(), inplace=True)
df['Marks'].fillna(df['Marks'].mean(), inplace=True)

print("\nData after Handling Missing Values:\n", df)

Conclusion:

Handling missing values ensures complete data for analysis and modeling. Using mean/median/mode imputation is simple and effective for numerical features.


---

ðŸ”¹ b. Noise Detection and Removal

Aim:

To detect and remove noisy (inconsistent/outlier) data for improved data quality.

Description:

Noise can occur due to faulty sensors, human error, or environmental factors. Noise removal techniques like smoothing, outlier detection, and clustering help in cleaning the dataset.

Procedure:

1. Load dataset.


2. Visualize or calculate outliers using IQR/Z-score.


3. Remove or replace noisy data.



Program (Using IQR):

# Continue from above or new dataset
Q1 = df['Marks'].quantile(0.25)
Q3 = df['Marks'].quantile(0.75)
IQR = Q3 - Q1

# Define outlier range
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Filter out noise
filtered_df = df[(df['Marks'] >= lower_bound) & (df['Marks'] <= upper_bound)]

print("\nData after Noise Removal:\n", filtered_df)

Conclusion:

Detecting and eliminating noisy data enhances the quality and reliability of data analysis. The IQR method is simple and effective for identifying outliers.


---

ðŸ”¹ c. Identifying Data Redundancy and Elimination

Aim:

To detect and eliminate duplicate or irrelevant data to reduce dataset size and improve efficiency.

Description:

Redundant data wastes storage and can mislead learning algorithms. Common redundancy includes duplicate rows or highly correlated columns.

Procedure:

1. Identify duplicate rows/columns.


2. Remove them.


3. Check feature correlation (for numeric data).


4. Remove highly correlated features if needed.



Program:

# Adding a duplicate row
df.loc[5] = df.loc[4]

# Detect duplicate rows
print("\nDuplicate Rows:\n", df[df.duplicated()])

# Remove duplicates
df_no_duplicates = df.drop_duplicates()
print("\nData after Removing Redundancy:\n", df_no_duplicates)

Conclusion:

Redundancy can degrade model accuracy and increase computational cost. Identifying and eliminating redundant entries helps maintain a clean and concise dataset.


---

2. Implement Any One Imputation Model

Letâ€™s implement KNN Imputation using the KNNImputer from Scikit-learn.


---

ðŸ”¹ Aim:

To fill missing values using the K-Nearest Neighbors (KNN) Imputation technique.


---

Description:

KNN Imputation replaces a missing value by finding the most similar k instances (neighbors) and averaging their non-missing values for the same feature. Itâ€™s more accurate than simple mean imputation when dealing with structured data.


---

Procedure:

1. Import and create dataset with missing values.


2. Initialize and apply KNNImputer.


3. Transform the dataset to fill missing values.


4. Display the updated dataset.




---

Program:

import pandas as pd
import numpy as np
from sklearn.impute import KNNImputer

# Sample dataset
data = {
    'Math': [85, np.nan, 78, 90, 88],
    'Science': [np.nan, 80, 75, np.nan, 85],
    'English': [78, 82, 84, 80, np.nan]
}
df = pd.DataFrame(data)
print("Original Data:\n", df)

# KNN Imputation
imputer = KNNImputer(n_neighbors=2)
df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)

print("\nData after KNN Imputation:\n", df_imputed)


---

Conclusion:

KNN Imputation considers feature similarity for filling missing values, leading to better predictions in many real-world scenarios compared to basic imputation. Itâ€™s more robust and intelligent for structured numeric datasets.


---

Let me know if you'd like this exported as a PDF report, or if you want a GUI-based version using tkinter or React!


Aim:
To demonstrate handling missing values in a dataset using Python. This includes identifying missing values and applying strategies such as imputation to deal with them.
Program:
We'll use the pandas library for data manipulation and the numpy library for handling numerical operations. The example will be based on a simple dataset created using a pandas DataFrame. We'll show how to identify missing values and use different methods to impute them.
Now, here's the Python program:
import pandas as pd

# Create a sample DataFrame with missing values
data = {
    'A': [1, 2, np.nan, 4, 5],
    'B': [5, np.nan, np.nan, 8, 10],
    'C': [10, 20, 30, 40, 50]
}
df = pd.DataFrame(data)

print("Original DataFrame:")
print(df)

# Identifying missing values
print("\nMissing Values in Each Column:")
print(df.isnull().sum())

# Method 1: Drop rows with any missing values
df_dropped = df.dropna()
print("\nDataFrame after dropping rows with missing values:")
print(df_dropped)

# Method 2: Impute missing values with a constant (e.g., 0)
df_filled_zero = df.fillna(0)
print("\nDataFrame after filling missing values with 0:")
print(df_filled_zero)

# Method 3: Impute missing values with mean of the column
df_filled_mean = df.fillna(df.mean())
print("\nDataFrame after filling missing values with column mean:")
print(df_filled_mean)

Output Explanation:
The original DataFrame is printed, showing the missing values (represented as NaN).
The program then displays the count of missing values in each column.
Three methods of handling missing values are demonstrated:
Dropping rows with any missing values.
Filling missing values with a constant (zero in this case).
Filling missing values with the mean of the respective column.
The output will show the DataFrame after each of these operations, demonstrating how missing values are handled.
Let's run this program to see the output.
The output of the program is as follows:


























b.  Noise detection removal

Aim:
The aim of this Python program is to demonstrate how to detect and remove noise from a dataset. Noise in data refers to irrelevant or meaningless information that can distort the actual signal in the dataset, leading to inaccuracies in analysis or model training. The focus will be on a simple example where noise is identified and filtered out from a numerical dataset.
Program:
We will use Python along with the pandas and numpy libraries for data manipulation. In this example, let's assume the noise is represented by unusually high or low values that deviate significantly from the overall pattern of the data. We'll use a basic statistical method where values outside a certain range (based on standard deviation) are considered noise.

import pandas as pd
importnumpy as np

# Create a sample DataFrame with some noise
data = {'Values': [12, 15, 22, 20, 19, 300, 18, 16, 13, 400, 14]}
df = pd.DataFrame(data)

print("Original DataFrame:")
print(df)

# Define a function to detect and filter out noise
defremove_noise(df, column, std_devs=2):
mean = df[column].mean()
std = df[column].std()
noise_threshold_upper = mean + (std_devs * std)
noise_threshold_lower = mean - (std_devs * std)
returndf[(df[column] <noise_threshold_upper) & (df[column] >noise_threshold_lower)]

# Remove noise from the DataFrame
df_cleaned = remove_noise(df, 'Values')

print("\nDataFrame after Noise Removal:")
print(df_cleaned)


Output Explanation:
The program starts by creating a DataFrame with a column of values, including some significantly higher values that represent noise.
A function remove_noise is defined, which calculates the mean and standard deviation of the specified column. It then filters out values that are more than a specified number of standard deviations away from the mean (default is 2, but this can be adjusted).
The function is applied to the DataFrame, and the cleaned DataFrame, with noise removed, is displayed.
Now, let's run the program to see the output.
It seems there was an issue with the execution of the program, specifically in displaying the output. Let me correct this and run the program again to obtain the expected output. 
The output of the program is as follows:






















c. Identifying data redundancy and elimination

Aim:
The aim of this Python program is to demonstrate the process of identifying and eliminating data redundancy from a dataset. Data redundancy refers to the unnecessary repetition of data within a dataset, which can lead to inefficient storage, processing, and potential inconsistencies. The focus will be on identifying duplicate rows in a dataset and removing them to create a more concise and efficient dataset.
Program:
We will use Python along with the pandas library for data manipulation. To simulate a dataset with redundancy, we'll create a DataFrame containing some duplicate rows. Then, we'll apply methods to detect and remove these duplicates.

import pandas as pd

# Create a sample DataFrame with duplicate rows
data = {
    'A': [1, 2, 3, 1, 2, 3],
    'B': [4, 5, 6, 4, 5, 6],
    'C': [7, 8, 9, 7, 8, 9]
}
df = pd.DataFrame(data)

print("Original DataFrame:")
print(df)

# Identifying duplicate rows
duplicates = df[df.duplicated()]

# Removing duplicate rows
df_unique = df.drop_duplicates()

print("\nDuplicate Rows:")
print(duplicates)

print("\nDataFrame after Removing Duplicates:")
print(df_unique)

Output Explanation:
The program creates a DataFramedf containing some rows that are exact duplicates of others.
It then identifies duplicate rows using the duplicated() method, which returns a boolean series indicating whether each row is a duplicate.
Finally, the program removes these duplicates using the drop_duplicates() method, resulting in a DataFramedf_unique without redundant data.
The output will display the original DataFrame, the identified duplicate rows, and the DataFrame after duplicate removal.
Let's execute the program to see the output.
The output of the program is as follows:



This example illustrates a straightforward method to detect and remove redundant data in a DataFrame using Python's pandas library. This process is crucial in data preprocessing to ensure the efficiency and accuracy of further data analysis or machine learning tasks.














2. Implement any one imputation model

Aim:
The aim of this Python program is to demonstrate how to build and apply an imputation model to handle missing values in a dataset. Imputation involves replacing missing or null values in a dataset with substitute values, which is crucial for maintaining the integrity of the dataset for analysis or machine learning models. We will use a simple statistical method for imputation, such as replacing missing values with the mean of the column.
Program:
We will use Python with the pandas and numpy libraries for data manipulation. Additionally, we'll use SimpleImputer from the sklearn.impute module for imputation.
First, ensure pandas, numpy, and scikit-learn are installed:

import pandas as pd
importnumpy as np
fromsklearn.impute import SimpleImputer

# Create a sample DataFrame with missing values
data = {'A': [1, 2, np.nan, 4], 'B': [np.nan, 2, 3, 4]}
df = pd.DataFrame(data)

print("Original DataFrame:")
print(df)

# Setting up the imputer model
imputer = SimpleImputer(strategy='mean')

# Applying the imputer model to the DataFrame
df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)

print("\nDataFrame after Imputation:")
print(df_imputed)

Output Explanation:
The program creates a DataFramedf with some missing values (np.nan).
A SimpleImputer model is initialized with the strategy 'mean', which means that missing values will be replaced with the mean of the respective column.
The imputer is applied to the DataFrame, resulting in df_imputed, which has all missing values filled with the mean of their respective columns.
The output will display the original DataFrame with missing values and the DataFrame after imputation.
Let's execute the program to see the output.
The output of the program is as follows:


This example illustrates how an imputation model can be used to handle missing data in a dataset. The SimpleImputer with a mean strategy is a basic approach, and depending on the dataset and the nature of the missing data, other strategies or more complex imputation models may be more appropriate. 




Hereâ€™s a complete and structured response for SET-1, including detailed explanation, code, and formatted sections for report or assignment submission.


---

âœ… SET-1


---

1. Data Preprocessing


---

ðŸ”¹ a. Handling Missing Values

Aim:

To identify and manage missing data in a dataset to ensure completeness and improve analysis accuracy.

Description:

Missing data can affect the performance of machine learning models and statistical analysis. These may arise due to errors in data collection or system faults. Techniques to handle missing data include deletion, imputation, or substitution.

Procedure:

1. Load the dataset.


2. Check for missing values.


3. Choose an appropriate method to handle them (drop or fill).


4. Apply the method and verify results.



Program:

import pandas as pd
import numpy as np

# Sample dataset
data = {
    'Name': ['Amit', 'Neha', 'John', 'Sara', 'Kiran'],
    'Age': [25, np.nan, 30, 22, np.nan],
    'Marks': [80, 90, np.nan, 85, 88]
}
df = pd.DataFrame(data)

print("Original Data:\n", df)

# Detect missing values
print("\nMissing Values:\n", df.isnull().sum())

# Fill missing values using mean
df['Age'].fillna(df['Age'].mean(), inplace=True)
df['Marks'].fillna(df['Marks'].mean(), inplace=True)

print("\nData after Handling Missing Values:\n", df)

Conclusion:

Handling missing values ensures complete data for analysis and modeling. Using mean/median/mode imputation is simple and effective for numerical features.


---

ðŸ”¹ b. Noise Detection and Removal

Aim:

To detect and remove noisy (inconsistent/outlier) data for improved data quality.

Description:

Noise can occur due to faulty sensors, human error, or environmental factors. Noise removal techniques like smoothing, outlier detection, and clustering help in cleaning the dataset.

Procedure:

1. Load dataset.


2. Visualize or calculate outliers using IQR/Z-score.


3. Remove or replace noisy data.



Program (Using IQR):

# Continue from above or new dataset
Q1 = df['Marks'].quantile(0.25)
Q3 = df['Marks'].quantile(0.75)
IQR = Q3 - Q1

# Define outlier range
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Filter out noise
filtered_df = df[(df['Marks'] >= lower_bound) & (df['Marks'] <= upper_bound)]

print("\nData after Noise Removal:\n", filtered_df)

Conclusion:

Detecting and eliminating noisy data enhances the quality and reliability of data analysis. The IQR method is simple and effective for identifying outliers.


---

ðŸ”¹ c. Identifying Data Redundancy and Elimination

Aim:

To detect and eliminate duplicate or irrelevant data to reduce dataset size and improve efficiency.

Description:

Redundant data wastes storage and can mislead learning algorithms. Common redundancy includes duplicate rows or highly correlated columns.

Procedure:

1. Identify duplicate rows/columns.


2. Remove them.


3. Check feature correlation (for numeric data).


4. Remove highly correlated features if needed.



Program:

# Adding a duplicate row
df.loc[5] = df.loc[4]

# Detect duplicate rows
print("\nDuplicate Rows:\n", df[df.duplicated()])

# Remove duplicates
df_no_duplicates = df.drop_duplicates()
print("\nData after Removing Redundancy:\n", df_no_duplicates)

Conclusion:

Redundancy can degrade model accuracy and increase computational cost. Identifying and eliminating redundant entries helps maintain a clean and concise dataset.


---

2. Implement Any One Imputation Model

Letâ€™s implement KNN Imputation using the KNNImputer from Scikit-learn.


---

ðŸ”¹ Aim:

To fill missing values using the K-Nearest Neighbors (KNN) Imputation technique.


---

Description:

KNN Imputation replaces a missing value by finding the most similar k instances (neighbors) and averaging their non-missing values for the same feature. Itâ€™s more accurate than simple mean imputation when dealing with structured data.


---

Procedure:

1. Import and create dataset with missing values.


2. Initialize and apply KNNImputer.


3. Transform the dataset to fill missing values.


4. Display the updated dataset.




---

Program:

import pandas as pd
import numpy as np
from sklearn.impute import KNNImputer

# Sample dataset
data = {
    'Math': [85, np.nan, 78, 90, 88],
    'Science': [np.nan, 80, 75, np.nan, 85],
    'English': [78, 82, 84, 80, np.nan]
}
df = pd.DataFrame(data)
print("Original Data:\n", df)

# KNN Imputation
imputer = KNNImputer(n_neighbors=2)
df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)

print("\nData after KNN Imputation:\n", df_imputed)


---

Conclusion:

KNN Imputation considers feature similarity for filling missing values, leading to better predictions in many real-world scenarios compared to basic imputation. Itâ€™s more robust and intelligent for structured numeric datasets.


---

Let me know if you'd like this exported as a PDF report, or if you want a GUI-based version using tkinter or React!

